{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    PreTrainedTokenizerFast\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "import json\n",
    "import typer\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from utils import (plot_v_gene_distributions, \n",
    "                   set_seed, \n",
    "                   MetricCallback, \n",
    "                    )\n",
    "\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../predict'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from predict import main as predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in environment variables\n",
    "load_dotenv()\n",
    "\n",
    "dataset_path = os.getenv('DATASET_PATH')\n",
    "independent_dataset_1 = os.getenv('INDEPENDENT_DATASET_PATH_1')\n",
    "independent_dataset_2 = os.getenv('INDEPENDENT_DATASET_PATH_2')\n",
    "tokenizer_path = os.getenv('TOKENIZER_PATH')\n",
    "untrained_model_path = os.getenv('UNTRAINED_MODEL_PATH')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import original dataset to use for training\n",
    "antibody_dataset = pd.read_parquet(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len filtered doaslap 77990\n",
      "length of dataset after cleaning: 57249 sequences\n"
     ]
    }
   ],
   "source": [
    "# clean the dataset before training\n",
    "\n",
    "# create the full amino acid sequence of the antibodies by concatenating the various parts\n",
    "antibody_dataset['sequence_aa'] = antibody_dataset['fwr1_aa'] + antibody_dataset['cdr1_aa'] + antibody_dataset['fwr2_aa'] + antibody_dataset['cdr2_aa'] + antibody_dataset['fwr3_aa'] + antibody_dataset['cdr3_aa'] + antibody_dataset['fwr4_aa']\n",
    "\n",
    "# filter out data we do not want\n",
    "cleaned_dataset = antibody_dataset[\n",
    "    # Filter out light chains\n",
    "    (antibody_dataset['chain_type'] == 'H') \n",
    "    # Filter out vhh antibodies, which are smaller and a different structure\n",
    "    & (antibody_dataset['vhh'] == False)  \n",
    "    # Make sure there is a classification for the antibody, and only one classification, and it is not a weird useless one\n",
    "    & (antibody_dataset['classification'] != '')\n",
    "    & (~antibody_dataset['classification'].str.contains(';'))\n",
    "    & (~antibody_dataset['classification'].isin(['other', 'methods development'])) \n",
    "    # Get rid of sequences with X as an amino acid\n",
    "    & (~antibody_dataset['sequence_aa'].str.contains('X'))\n",
    "    # Make sure all regions of sequence are present and sensible length\n",
    "    & (antibody_dataset[\"fwr1_aa\"].str.len() >= 10)\n",
    "    & (antibody_dataset[\"fwr2_aa\"].str.len() >= 1)\n",
    "    & (antibody_dataset[\"fwr3_aa\"].str.len() >= 1)\n",
    "    & (antibody_dataset[\"fwr4_aa\"].str.len() >= 10)\n",
    "    & (antibody_dataset[\"cdr1_aa\"].str.len() >= 5)\n",
    "    & (antibody_dataset[\"cdr1_aa\"].str.len() <= 12)\n",
    "    & (antibody_dataset[\"cdr2_aa\"].str.len() >= 3)\n",
    "    & (antibody_dataset[\"cdr2_aa\"].str.len() <= 10)\n",
    "    & (antibody_dataset[\"cdr3_aa\"].str.len() >= 5)\n",
    "    & (antibody_dataset[\"cdr3_aa\"].str.len() <= 30)\n",
    "    # Remove sequences from the PDB, as fabcon was already trained on these\n",
    "    # & (antibody_dataset['is_source_pdb']!=True)\n",
    "    # Then remove duplicates of the CDR1, 2, and 3 - leaving these in risks data leakage during training\n",
    "].drop_duplicates(subset=['cdr1_aa', 'cdr2_aa', 'cdr3_aa'])\n",
    "print(\"len filtered doaslap\", len(cleaned_dataset))\n",
    "\n",
    "# create a v_gene column which contains the v gene instead of the allele\n",
    "# by removing everything after a * (which would signify the allele)\n",
    "cleaned_dataset['v_gene'] = cleaned_dataset['v_call'].str.split(\"*\", n=1).str[0]\n",
    "\n",
    "# get human antibody sequences\n",
    "human_dataset = cleaned_dataset[   \n",
    "    (\n",
    "        (cleaned_dataset['annotations_organism'] == 'Homo sapiens') |\n",
    "        (cleaned_dataset['v_call_species'] == 'human')\n",
    "    )\n",
    "    & (cleaned_dataset['classification'].isin(['self-protein', 'peptide']))\n",
    "]\n",
    "\n",
    "# get viral antibody sequences\n",
    "viral_dataset = cleaned_dataset[(cleaned_dataset['classification'] == 'viral')]\n",
    "\n",
    "# remove PDB data since it was already used in fabcon\n",
    "human_dataset=human_dataset[human_dataset['is_source_pdb']==False]\n",
    "viral_dataset=viral_dataset[viral_dataset['is_source_pdb']==False]\n",
    "\n",
    "# add labels for the machine learning binary classification\n",
    "human_dataset['label'] = 0\n",
    "viral_dataset['label'] = 1\n",
    "\n",
    "\n",
    "print('length of dataset after cleaning:', (pd.concat([human_dataset, viral_dataset])).shape[0], 'sequences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced dataset size: 46354\n"
     ]
    }
   ],
   "source": [
    "# make a dataset which is 50/50 human/viral\n",
    "\n",
    "min_length_data = min(len(human_dataset), len(viral_dataset))\n",
    "\n",
    "df = pd.concat([human_dataset.iloc[:min_length_data],viral_dataset.iloc[:min_length_data]])\n",
    "\n",
    "print('balanced dataset size:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    }
   ],
   "source": [
    "# split and tokenize the data\n",
    "def stratified_group_split(df, stratify_col, group_col, random_state=4):\n",
    "    # Create a combined stratification column that uses both v_gene and cdr3_aa\n",
    "    df['strat_group'] = df[stratify_col].astype(str) + \"_\" + df[group_col].astype(str)\n",
    "    \n",
    "    # Use GroupShuffleSplit to ensure groups remain intact\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "    train_idx, test_val_idx = next(splitter.split(df, groups=df[\"strat_group\"]))\n",
    "\n",
    "    df_test_val = df.iloc[test_val_idx]\n",
    "    \n",
    "    # Split the test_val set into test and validation\n",
    "    splitter_val = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=random_state)\n",
    "    val_idx, test_idx = next(splitter_val.split(df_test_val, groups=df_test_val[\"strat_group\"]))\n",
    "    \n",
    "    # Create the final dataframes\n",
    "    df_train = df.iloc[train_idx].copy()\n",
    "    df_val = df_test_val.iloc[val_idx].copy()\n",
    "    df_test = df_test_val.iloc[test_idx].copy()\n",
    "    \n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "df_train, df_val, df_test = stratified_group_split(\n",
    "    df,\n",
    "    stratify_col='cdr3_aa',\n",
    "    group_col='v_gene',\n",
    "    random_state=4\n",
    ")\n",
    "\n",
    "df_train['sequence_aa'] = 'á¸¢'+df_train['sequence_aa']\n",
    "df_val['sequence_aa'] = 'á¸¢'+df_val['sequence_aa']\n",
    "df_test['sequence_aa'] = 'á¸¢'+df_test['sequence_aa']\n",
    "\n",
    "df_train.to_parquet('train.parquet')\n",
    "df_val.to_parquet('val.parquet')\n",
    "df_test.to_parquet('test.parquet')\n",
    "\n",
    "\n",
    "dd = datasets.DatasetDict({\n",
    "    \"train\": datasets.Dataset.from_pandas(df_train),\n",
    "    \"eval\": datasets.Dataset.from_pandas(df_val),\n",
    "    \"test\": datasets.Dataset.from_pandas(df_test)\n",
    "})\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "   tokenizer_path,\n",
    "    padding=True,\n",
    "    max_len=256\n",
    ")\n",
    "\n",
    "dd_tokenized = dd.map(\n",
    "    lambda z:tokenizer(\n",
    "        z['sequence_aa'],\n",
    "        padding=True\n",
    "    ),\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    remove_columns=['sequence_aa',\n",
    "                    '_seq',\n",
    "                    'id',\n",
    "                    \"strat_group\",\n",
    "                    'name',\n",
    "                    'description',\n",
    "                    'dbxrefs',\n",
    "                    'annotations_date',\n",
    "                    'annotations_db_source',\n",
    "                    'annotations_source',\n",
    "                    'annotations_organism',\n",
    "                    'annotations_comment',\n",
    "                    'protein_id',\n",
    "                    'titles',\n",
    "                    'pubmed_ids',\n",
    "                    'journal',\n",
    "                    'remarks',\n",
    "                    'patent_sequence',\n",
    "                    'patent_id',\n",
    "                    'is_source_pdb',\n",
    "                    'fwr1_aa',\n",
    "                    'cdr1_aa',\n",
    "                    'fwr2_aa',\n",
    "                    'cdr2_aa',\n",
    "                    'fwr3_aa',\n",
    "                    'cdr3_aa',\n",
    "                    'fwr4_aa',\n",
    "                    'junction_aa',\n",
    "                    'v_call',\n",
    "                    'j_call',\n",
    "                    'v_call_score',\n",
    "                    'v_call_species',\n",
    "                    'j_call_score',\n",
    "                    'j_call_species',\n",
    "                    'species',\n",
    "                    'chain_type',\n",
    "                    'tail',\n",
    "                    'species_latin',\n",
    "                    'v_region_mismatch',\n",
    "                    'targets_from_titles',\n",
    "                    'genes_from_titles',\n",
    "                    'classification_from_titles',\n",
    "                    'targets_from_abstracts',\n",
    "                    'genes_from_abstracts',\n",
    "                    'classification_from_abstracts',\n",
    "                    'targets_from_pub_abstracts',\n",
    "                    'genes_from_pub_abstracts',\n",
    "                    'classification_from_pub_abstracts',\n",
    "                    'vhh',\n",
    "                    'gene_list',\n",
    "                    'uniprot_id',\n",
    "                    'gene',\n",
    "                    'target',\n",
    "                    'classification',\n",
    "                    'v_gene',\n",
    "                    '__index_level_0__'\n",
    "                ]\n",
    ").remove_columns('token_type_ids')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    }
   ],
   "source": [
    "# save the tokenized data to disk\n",
    "dd_tokenized.save_to_disk(\"datasetdict_tokenized_labelled.set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an optional step to check that v gene distributions are roughly the same across the 3 datasets\n",
    "plot_v_gene_distributions(df_train, df_val, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FalconForSequenceClassification were not initialized from the model checkpoint at /home/ubuntu/data/models and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ubuntu/miniconda3/envs/ml/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Train length: 37243\n",
      "Eval length: 4625\n",
      "Test length: 4486\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='23280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7500/23280 33:28 < 1:10:27, 3.73 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Aupr</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.490700</td>\n",
       "      <td>0.545678</td>\n",
       "      <td>0.857540</td>\n",
       "      <td>0.661942</td>\n",
       "      <td>0.747152</td>\n",
       "      <td>0.889282</td>\n",
       "      <td>0.889416</td>\n",
       "      <td>0.564689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>0.413127</td>\n",
       "      <td>0.859070</td>\n",
       "      <td>0.738402</td>\n",
       "      <td>0.794179</td>\n",
       "      <td>0.903925</td>\n",
       "      <td>0.904957</td>\n",
       "      <td>0.621282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.467800</td>\n",
       "      <td>0.451856</td>\n",
       "      <td>0.851870</td>\n",
       "      <td>0.743557</td>\n",
       "      <td>0.794037</td>\n",
       "      <td>0.899604</td>\n",
       "      <td>0.904630</td>\n",
       "      <td>0.617059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.440200</td>\n",
       "      <td>0.401176</td>\n",
       "      <td>0.847203</td>\n",
       "      <td>0.774055</td>\n",
       "      <td>0.808979</td>\n",
       "      <td>0.907328</td>\n",
       "      <td>0.907462</td>\n",
       "      <td>0.634597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.519409</td>\n",
       "      <td>0.924906</td>\n",
       "      <td>0.634880</td>\n",
       "      <td>0.752929</td>\n",
       "      <td>0.923037</td>\n",
       "      <td>0.924962</td>\n",
       "      <td>0.612599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.384856</td>\n",
       "      <td>0.814909</td>\n",
       "      <td>0.826460</td>\n",
       "      <td>0.820644</td>\n",
       "      <td>0.909752</td>\n",
       "      <td>0.912222</td>\n",
       "      <td>0.636338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.381700</td>\n",
       "      <td>0.362010</td>\n",
       "      <td>0.856550</td>\n",
       "      <td>0.797680</td>\n",
       "      <td>0.826068</td>\n",
       "      <td>0.920725</td>\n",
       "      <td>0.919416</td>\n",
       "      <td>0.663568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.369300</td>\n",
       "      <td>0.378326</td>\n",
       "      <td>0.855383</td>\n",
       "      <td>0.805412</td>\n",
       "      <td>0.829646</td>\n",
       "      <td>0.918610</td>\n",
       "      <td>0.920408</td>\n",
       "      <td>0.668299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.357500</td>\n",
       "      <td>0.390116</td>\n",
       "      <td>0.801378</td>\n",
       "      <td>0.899485</td>\n",
       "      <td>0.847602</td>\n",
       "      <td>0.925408</td>\n",
       "      <td>0.925299</td>\n",
       "      <td>0.679282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.356629</td>\n",
       "      <td>0.848857</td>\n",
       "      <td>0.861254</td>\n",
       "      <td>0.855011</td>\n",
       "      <td>0.934184</td>\n",
       "      <td>0.934903</td>\n",
       "      <td>0.705980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.345900</td>\n",
       "      <td>0.347322</td>\n",
       "      <td>0.891366</td>\n",
       "      <td>0.807131</td>\n",
       "      <td>0.847160</td>\n",
       "      <td>0.935063</td>\n",
       "      <td>0.934288</td>\n",
       "      <td>0.710202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.330600</td>\n",
       "      <td>0.337011</td>\n",
       "      <td>0.874494</td>\n",
       "      <td>0.835052</td>\n",
       "      <td>0.854318</td>\n",
       "      <td>0.931784</td>\n",
       "      <td>0.932060</td>\n",
       "      <td>0.714108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.323100</td>\n",
       "      <td>0.344115</td>\n",
       "      <td>0.815171</td>\n",
       "      <td>0.909364</td>\n",
       "      <td>0.859695</td>\n",
       "      <td>0.935774</td>\n",
       "      <td>0.936321</td>\n",
       "      <td>0.705742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.327100</td>\n",
       "      <td>0.390530</td>\n",
       "      <td>0.874662</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.853498</td>\n",
       "      <td>0.932940</td>\n",
       "      <td>0.933379</td>\n",
       "      <td>0.712885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>0.342063</td>\n",
       "      <td>0.831059</td>\n",
       "      <td>0.889605</td>\n",
       "      <td>0.859336</td>\n",
       "      <td>0.936093</td>\n",
       "      <td>0.937186</td>\n",
       "      <td>0.708444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "def train_fabcon(\n",
    "    ds: str = typer.Option(..., help=\"HF dataset with train, eval, and test splits\"),\n",
    "    pretrained_model_path: str = typer.Option(..., help = \"Path to pretrained model\"),\n",
    "    tokenizer_path: str = typer.Option(..., help=\"Path to the tokenizer\"),\n",
    "    freeze_encoder: bool = typer.Option(True, help = \"Specify whether or not to freeze the en(de)coder\"),\n",
    "    checkpoint_path: str = typer.Option(\"classifier-checkpoints/\", help=\"Path to save checkpoints\"),\n",
    "    model_save_path: str = typer.Option(\"classifier-model/\", help=\"Path to save the final model\"),\n",
    "    logging_path: str = typer.Option(\"tensorboard/\", help=\"Path to save Tensorboard logging\"),\n",
    "    metrics_path: str = typer.Option(\".\", help=\"Specify a folder path to write test metrics and logs\"),\n",
    "    seed: int = typer.Option(42, help = \"Set random seed.\"),\n",
    "    task_name: str = typer.Option(\"0\", help = 'Task Name'),\n",
    "    use_fp16: bool = typer.Option(False, help = 'Specify to use 16-bit precision instead of 32-bit precision'),\n",
    "    batch_size: int = typer.Option(64, help = 'Specify batch size'),\n",
    "    grad_acc: int = typer.Option(1, help = 'Specify number of gradient accumulation steps'),\n",
    "    epochs: int = typer.Option(3, help = 'Specify number of epochs'),\n",
    "    warmup_ratio: float = typer.Option(0.05, help = 'Specify warmup ratio'),\n",
    "    learning_rate: float = typer.Option(0.0001, help = 'Specify learning rate'),\n",
    "    no_cuda: bool = typer.Option(False, help = 'Specify True to NOT use CUDA'),\n",
    "    deepspeed: str = typer.Option(None, help=\"Specify deepspeed config path\")\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train a sequence classifier from RoFormer models.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_path, \n",
    "                                                               num_labels=2, \n",
    "                                                               trust_remote_code=True)\n",
    "#     print(model.config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path,\n",
    "                                              max_len=model.config.max_position_embeddings)\n",
    "\n",
    "  # Freeze all layers except the classifier layer\n",
    "    if freeze_encoder:\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'classifier' not in name:\n",
    "                param.requires_grad = False\n",
    "        # \n",
    "        # if 'Falcon' in model.config.architectures[0]:\n",
    "        #     for name, param in model.named_parameters():\n",
    "        #         if 'score' in name:\n",
    "        #             param.requires_grad = True\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "    tokenized_dataset = load_from_disk(ds)\n",
    "\n",
    "    \n",
    "    print(\"----------------------------------------------\")\n",
    "    print(f\"Train length: {tokenized_dataset['train'].num_rows}\")\n",
    "    print(f\"Eval length: {tokenized_dataset['eval'].num_rows}\")\n",
    "    print(f\"Test length: {tokenized_dataset['test'].num_rows}\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=checkpoint_path,\n",
    "    overwrite_output_dir=True,\n",
    "    logging_dir=logging_path,\n",
    "    report_to='all',\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_acc,\n",
    "    num_train_epochs=epochs,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type='cosine',\n",
    "    seed=seed,\n",
    "    weight_decay=0.05,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    # evaluation_strategy='epoch',  # evaluate every epoch\n",
    "    # save_strategy='epoch',  # save model every epoch\n",
    "    save_strategy='steps',  \n",
    "    metric_for_best_model='auc',  # monitor the AUC metric\n",
    "    load_best_model_at_end=True,  # load the best model after training\n",
    "    fp16=use_fp16,\n",
    "    no_cuda=no_cuda,\n",
    "    deepspeed=deepspeed\n",
    ")\n",
    "    early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # number of evaluations to wait before stopping\n",
    "    early_stopping_threshold=0.01  # minimum change to qualify as improvement\n",
    "    )\n",
    "\n",
    "    mc = MetricCallback(label_names=['negative', 'positive'])\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['eval'],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=mc.compute_metrics,\n",
    "        callbacks=[early_stopping]  \n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_save_path)\n",
    "\n",
    "    test_metrics = trainer.predict(tokenized_dataset['test'])\n",
    "    outpath = Path(f\"{metrics_path}\",\n",
    "                f\"{task_name}-metrics\",\n",
    "                f\"{task_name}.{seed}.test_metrics.json\")\n",
    "\n",
    "    outpath_pred = Path(f\"{metrics_path}\",\n",
    "                f\"{task_name}-metrics\",\n",
    "                f\"{task_name}.{seed}.test_predictions.csv\")\n",
    "\n",
    "    outpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with outpath.open('w') as jason:\n",
    "        json.dump(test_metrics.metrics, jason)\n",
    "\n",
    "    test_logits = torch.Tensor(test_metrics.predictions).softmax(dim=1).numpy()\n",
    "    test_labels = test_metrics.label_ids\n",
    "\n",
    "    dat = pd.DataFrame(test_logits, columns = ['neg','pos'])\n",
    "    dat['label'] = test_labels\n",
    "    dat.to_csv(outpath_pred, index=False)\n",
    "\n",
    "train_fabcon(ds=\"../model/datasetdict_tokenized_labelled.set\", \n",
    "     pretrained_model_path=\"/home/ubuntu/data/models\", \n",
    "     tokenizer_path=tokenizer_path,\n",
    "     freeze_encoder=False,\n",
    "     checkpoint_path=\"../model/trained_model/classifier-checkpoints\",\n",
    "     model_save_path=\"../model/trained_model/classifier-model\",\n",
    "     logging_path=\"../model/trained_model/tensorboard\",\n",
    "     metrics_path=\"../model/trained_model\",\n",
    "     seed=30,\n",
    "     warmup_ratio=0.05,\n",
    "     learning_rate=0.0001,\n",
    "     task_name=\"0\",\n",
    "     use_fp16=False,\n",
    "     batch_size=16,\n",
    "     grad_acc=1,\n",
    "     epochs=10,\n",
    "     no_cuda=False,\n",
    "     deepspeed=None\n",
    "     )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997\n"
     ]
    }
   ],
   "source": [
    "# combine the two independent datasets\n",
    "\n",
    "# Read the files\n",
    "csv_df = pd.read_csv(independent_dataset_1, usecols=['vh_full', 'label'])\n",
    "tsv_df = pd.read_csv(independent_dataset_2, sep='\\t', usecols=['Heavy Sequence', 'Label'])\n",
    "\n",
    "\n",
    "tsv_df['Heavy Sequence']\n",
    "csv_df['vh_full']\n",
    "# # Rename columns for consistency\n",
    "csv_df = csv_df.rename(columns={'vh_full': 'sequence_vh', 'label': 'label'})\n",
    "tsv_df = tsv_df.rename(columns={'Heavy Sequence': 'sequence_vh', 'Label': 'label'})\n",
    "\n",
    "# # Concatenate dataframes\n",
    "independent_df = pd.concat([csv_df, tsv_df], ignore_index=True)\n",
    "\n",
    "independent_df = independent_df[~independent_df['sequence_vh'].isin(df['sequence_aa'])]\n",
    "\n",
    "independent_df = independent_df.dropna()\n",
    "\n",
    "independent_df['label'] = independent_df['label'].str.lower().apply(\n",
    "    lambda x: 0 if 'human' in x else (1 if 'viral' in x else x)\n",
    ")\n",
    "\n",
    "independent_df = independent_df.drop_duplicates(keep='first')\n",
    "independent_df = independent_df[independent_df['label'].isin([0, 1])]\n",
    "\n",
    "independent_df.to_csv('./independent_dataset.csv', index=False)\n",
    "\n",
    "\n",
    "# First, get the counts of each label\n",
    "label_counts = independent_df['label'].value_counts()\n",
    "\n",
    "# Get the minimum count between the two labels\n",
    "min_count = min(label_counts[0], label_counts[1])\n",
    "\n",
    "balanced_independent_df = (\n",
    "    independent_df\n",
    "    .groupby('label')\n",
    "    .apply(lambda x: x.sample(min_count, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "balanced_independent_df.to_csv('./balanced_independent_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import os \n",
    "from datasets import (\n",
    "    load_from_disk,\n",
    "    DatasetDict, \n",
    "    Dataset\n",
    ")\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerFast,\n",
    "    FalconForSequenceClassification,\n",
    "    Trainer,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    recall_score, \n",
    "    precision_score, \n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "from utils import get_full_aa_sub\n",
    "\n",
    "class AntibodyRepertoireDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.sequences[index]\n",
    "\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    print('RANK',rank)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def get_eos_embedding(logits, input_ids, pad_token_id, rank):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sequence_lengths = torch.eq(input_ids, pad_token_id).int().argmax(-1) - 1\n",
    "    sequence_lengths = sequence_lengths % input_ids.shape[-1]\n",
    "    sequence_lengths = sequence_lengths.to(rank)\n",
    "    pooled_logits = logits[torch.arange(input_ids.shape[0], device=rank), sequence_lengths]\n",
    "    return pooled_logits\n",
    "# def get_eos_embedding(hidden_states, input_ids, pad_token_id, rank):\n",
    "#     # Get attention mask based on pad token\n",
    "#     attention_mask = (input_ids != pad_token_id).long()\n",
    "    \n",
    "#     # Find the last non-padded position for each sequence\n",
    "#     last_positions = attention_mask.sum(dim=1) - 1  # subtract 1 to get 0-based index\n",
    "    \n",
    "#     # Ensure indices are valid\n",
    "#     batch_size, seq_length = input_ids.size()\n",
    "#     last_positions = torch.clamp(last_positions, 0, seq_length - 1)\n",
    "    \n",
    "#     # Get batch indices\n",
    "#     batch_indices = torch.arange(batch_size, device=rank)\n",
    "    \n",
    "#     # Extract the embeddings for the last token of each sequence\n",
    "#     last_token_embeddings = hidden_states[batch_indices, last_positions]\n",
    "    \n",
    "#     return last_token_embeddings\n",
    "    \n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def predict(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    tokenizer_path: str,\n",
    "    model_path: str,\n",
    "    local_rank: int = 0,\n",
    "    world_size: int = 1\n",
    "    # vh_column: str = typer.Option(\"sequence_vh\", help =\"Column in input file which contains the heavy chain sequence\")\n",
    "):\n",
    "    setup(local_rank,\n",
    "          world_size)\n",
    "    # dist.init_process_group(backend='nccl')\n",
    "    # torch.cuda.set_device(local_rank)\n",
    "    if input_path.endswith('.csv'):\n",
    "        #LOAD DATASET\n",
    "        dataset_to_predict=pd.read_csv(input_path)\n",
    "        original_columns = dataset_to_predict.copy()\n",
    "\n",
    "        available_columns=dataset_to_predict.columns\n",
    "        \n",
    "        if 'label' in available_columns:\n",
    "            dataset_to_predict=dataset_to_predict[['sequence_vh','label']].copy()\n",
    "            dataset_to_predict['label'] = dataset_to_predict['label'].astype(int)\n",
    "        else:\n",
    "            dataset_to_predict=dataset_to_predict[['sequence_vh']].copy()\n",
    "    \n",
    "    # output from pipeline is tsv files\n",
    "    elif input_path.endswith('.tsv'):\n",
    "        dataset_to_predict=pd.read_csv(input_path, sep='\\t')  \n",
    "        available_columns=dataset_to_predict.columns\n",
    "        if 'sequence_vh' in available_columns:\n",
    "            raise NameError('tsv file already has sequence_vh column. Please remove it to continue')\n",
    "        elif 'sequence_alignment' not in available_columns and 'germline_alignment_d_mask' not in available_columns:\n",
    "            raise NameError('tsv file must have sequence_alignment and germline_alignment_d_mask columns') \n",
    "        else: \n",
    "            dataset_to_predict['sequence_vh'] = dataset_to_predict.apply(\n",
    "            lambda row: get_full_aa_sub(str(row['sequence_alignment']), str(row['germline_alignment_d_mask'])), \n",
    "            axis=1\n",
    "            )\n",
    "            original_columns = dataset_to_predict.copy()\n",
    "\n",
    "\n",
    "    #PREPARE DATASET INTO HF FORMAT\n",
    "    dataset_to_predict['sequence']='á¸¢'+dataset_to_predict['sequence_vh']\n",
    "    # Dataset is a HuggingFace dataset object that can be converted from Pandas.\n",
    "    # dd = DatasetDict({\n",
    "    #     \"test\": Dataset.from_pandas(dataset_to_predict)\n",
    "    # })\n",
    "    \n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
    "\n",
    "    def collate_fn(text, tokenizer=tokenizer):\n",
    "        # pads to max length in batch\n",
    "        tokenized_input = tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            max_length=256\n",
    "        )\n",
    "        tokenized_input['text'] = text\n",
    "        return tokenized_input\n",
    "    model = FalconForSequenceClassification.from_pretrained(model_path).to(local_rank)\n",
    "    # model = FalconForSequenceClassification.from_pretrained(model_path).eval()\n",
    "\n",
    "    # model = model.eval()\n",
    "    model = DDP(model, device_ids=[local_rank])\n",
    "\n",
    "    sequences = dataset_to_predict['sequence'].unique().tolist()\n",
    "\n",
    "    dataset = AntibodyRepertoireDataset(sequences)\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=local_rank)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=16, sampler=sampler,\n",
    "                            collate_fn=collate_fn)\n",
    "    pad_token_id = 2\n",
    "\n",
    "    sequences = []\n",
    "    viral_probabilities = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(local_rank),\n",
    "                'attention_mask': batch['attention_mask'].to(local_rank)\n",
    "            }\n",
    "\n",
    "            o = model(**inputs).logits\n",
    "            probs = torch.softmax(o.cpu(), dim=1).detach().numpy()  \n",
    "            \n",
    "            probs = probs[:, -1] \n",
    "            # print(probs)\n",
    "            # print(batch['text'])\n",
    "            # sequences are stored in batch['text']\n",
    "            # probabilities are stored in probs\n",
    "            # I want to connect 1 sequence to 1 probability\n",
    "            # print(tuple(zip(batch['text'], probs)))\n",
    "            # outputs.append(o.cpu())\n",
    "            viral_probabilities.extend(probs)\n",
    "            sequences.extend(batch['text'])\n",
    "    output_df = pd.DataFrame({\n",
    "        'sequence_vh': sequences,\n",
    "        'viral_prob': viral_probabilities\n",
    "    })\n",
    "\n",
    "    merged_df = dataset_to_predict.merge(\n",
    "    output_df,\n",
    "    left_on='sequence',\n",
    "    right_on='sequence_vh',\n",
    "    how='left'\n",
    ")\n",
    "    merged_df['human_prob'] = 1 - merged_df['viral_prob']\n",
    "    # Define conditions\n",
    "    conditions = [\n",
    "        merged_df['viral_prob'] > 0.95,\n",
    "        merged_df['human_prob'] > 0.95\n",
    "    ]\n",
    "\n",
    "    # Define choices for each condition\n",
    "    choices = ['viral', 'human']\n",
    "\n",
    "    # Apply conditions to create 'human_viral_prediction' column\n",
    "    merged_df['human_viral_prediction'] = np.select(\n",
    "        conditions, choices, default=\"\"\n",
    "    )\n",
    "    print(merged_df)\n",
    "\n",
    "    if \".csv\" in output_path:\n",
    "        # final_output = pd.merge(\n",
    "        #     original_columns,\n",
    "        #     dataset_to_predict[['sequence_vh', 'viral_prob', 'human_prob', 'human_viral_prediction']],\n",
    "        #     on='sequence_vh',\n",
    "        #     how='left'\n",
    "        # )\n",
    "        merged_df.to_csv(output_path,index=None)\n",
    "    elif \".tsv\" in output_path:\n",
    "        # original_columns['sequence_vh'] = original_columns.apply(\n",
    "        #     lambda row: get_full_aa_sub(str(row['sequence_alignment']), str(row['germline_alignment_d_mask'])), \n",
    "        #     axis=1\n",
    "        # )\n",
    "        # final_output = pd.merge(\n",
    "        #     original_columns,\n",
    "        #     dataset_to_predict[['sequence_vh', 'viral_prob', 'human_prob', 'human_viral_prediction']],\n",
    "        #     on='sequence_vh',\n",
    "        #     how='left'\n",
    "        # )\n",
    "        merged_df.to_csv(output_path,index=None,sep=\"\\t\")\n",
    "\n",
    "    # viral_probabilities = merged_df['viral_probs'].values\n",
    "    if 'label' in available_columns:\n",
    "        labels=merged_df['label'].values\n",
    "        probs=merged_df['viral_prob'].values\n",
    "        probs_binary=[1 if x>0.5 else 0 for x in viral_probabilities]\n",
    "        # print(labels)\n",
    "        # print(probs)\n",
    "        auc=roc_auc_score(labels, probs)\n",
    "        aupr=average_precision_score(labels, probs)\n",
    "        f1=f1_score(labels,probs_binary)\n",
    "        precision=precision_score(labels,probs_binary)\n",
    "        recall=recall_score(labels,probs_binary)\n",
    "        mcc = matthews_corrcoef(labels, probs_binary)\n",
    "        \n",
    "        print('roc_auc:',auc,'average_precision_score',aupr,'f1:',f1,'precision:',precision,'recall:',recall,'mcc',mcc)\n",
    "\n",
    "    cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANK 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/ml/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2849: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77/77 [00:04<00:00, 16.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          sequence_vh_x  label  \\\n",
      "0     EVQLLESGGGLVQPGGSLRLSCAASGFTLSTHAMSWVRQAPGKGLE...      0   \n",
      "1     EVQLVQSGAEVKKPGESLKISCKGSGYSFTNYWVGWVRQMPGKGLE...      0   \n",
      "2     VQLVESGGGLVQPGGSLRLSCAASGFNISSSSIHWVRQAPGKGLEW...      0   \n",
      "3     EVQLLESGGGFLQPGGSLRLSCAASGFTFSNYAMSWVRQAPGKGLE...      0   \n",
      "4     SEHLVESGGGLVKPGGSLRLSCATADFFEYDDMSWVRQAPGKGLEW...      0   \n",
      "...                                                 ...    ...   \n",
      "1221  QVQLVQSGAEVKKPGSSVKVSCKTSGGTFSSYAISWVRQAPGQGLE...      1   \n",
      "1222  EVQLVQSGAEVKKPGESLKISCKGSGYSFTSNWIGWVRQMPGKGLE...      1   \n",
      "1223  EVQLLESGGGLVQPGGSLRLSCAASGFTFTSYGMSWVRQAPGKGLE...      1   \n",
      "1224  EVQLVESGGGLVQPGGSLRLSCAASGFTVSSNYMSWVRQAPGKGLE...      1   \n",
      "1225  EVQLVESGGGLVKPGGSLRLSCAASGFTFSSYYMNWVRQAPGKGLE...      1   \n",
      "\n",
      "                                               sequence  \\\n",
      "0     á¸¢EVQLLESGGGLVQPGGSLRLSCAASGFTLSTHAMSWVRQAPGKGL...   \n",
      "1     á¸¢EVQLVQSGAEVKKPGESLKISCKGSGYSFTNYWVGWVRQMPGKGL...   \n",
      "2     á¸¢VQLVESGGGLVQPGGSLRLSCAASGFNISSSSIHWVRQAPGKGLE...   \n",
      "3     á¸¢EVQLLESGGGFLQPGGSLRLSCAASGFTFSNYAMSWVRQAPGKGL...   \n",
      "4     á¸¢SEHLVESGGGLVKPGGSLRLSCATADFFEYDDMSWVRQAPGKGLE...   \n",
      "...                                                 ...   \n",
      "1221  á¸¢QVQLVQSGAEVKKPGSSVKVSCKTSGGTFSSYAISWVRQAPGQGL...   \n",
      "1222  á¸¢EVQLVQSGAEVKKPGESLKISCKGSGYSFTSNWIGWVRQMPGKGL...   \n",
      "1223  á¸¢EVQLLESGGGLVQPGGSLRLSCAASGFTFTSYGMSWVRQAPGKGL...   \n",
      "1224  á¸¢EVQLVESGGGLVQPGGSLRLSCAASGFTVSSNYMSWVRQAPGKGL...   \n",
      "1225  á¸¢EVQLVESGGGLVKPGGSLRLSCAASGFTFSSYYMNWVRQAPGKGL...   \n",
      "\n",
      "                                          sequence_vh_y  viral_prob  \\\n",
      "0     á¸¢EVQLLESGGGLVQPGGSLRLSCAASGFTLSTHAMSWVRQAPGKGL...    0.018584   \n",
      "1     á¸¢EVQLVQSGAEVKKPGESLKISCKGSGYSFTNYWVGWVRQMPGKGL...    0.521846   \n",
      "2     á¸¢VQLVESGGGLVQPGGSLRLSCAASGFNISSSSIHWVRQAPGKGLE...    0.013029   \n",
      "3     á¸¢EVQLLESGGGFLQPGGSLRLSCAASGFTFSNYAMSWVRQAPGKGL...    0.080175   \n",
      "4     á¸¢SEHLVESGGGLVKPGGSLRLSCATADFFEYDDMSWVRQAPGKGLE...    0.012041   \n",
      "...                                                 ...         ...   \n",
      "1221  á¸¢QVQLVQSGAEVKKPGSSVKVSCKTSGGTFSSYAISWVRQAPGQGL...    0.996160   \n",
      "1222  á¸¢EVQLVQSGAEVKKPGESLKISCKGSGYSFTSNWIGWVRQMPGKGL...    0.865250   \n",
      "1223  á¸¢EVQLLESGGGLVQPGGSLRLSCAASGFTFTSYGMSWVRQAPGKGL...    0.146407   \n",
      "1224  á¸¢EVQLVESGGGLVQPGGSLRLSCAASGFTVSSNYMSWVRQAPGKGL...    0.760678   \n",
      "1225  á¸¢EVQLVESGGGLVKPGGSLRLSCAASGFTFSSYYMNWVRQAPGKGL...    0.318147   \n",
      "\n",
      "      human_prob human_viral_prediction  \n",
      "0       0.981416                  human  \n",
      "1       0.478154                         \n",
      "2       0.986971                  human  \n",
      "3       0.919825                         \n",
      "4       0.987959                  human  \n",
      "...          ...                    ...  \n",
      "1221    0.003840                  viral  \n",
      "1222    0.134750                         \n",
      "1223    0.853593                         \n",
      "1224    0.239322                         \n",
      "1225    0.681853                         \n",
      "\n",
      "[1226 rows x 7 columns]\n",
      "roc_auc: 0.7832338484547688 average_precision_score 0.6661898713989832 f1: 0.5320665083135392 precision: 0.5169230769230769 recall: 0.5481239804241436 mcc 0.03595462477533374\n"
     ]
    }
   ],
   "source": [
    "# These two environment variables are necessary to get the GPU-ified version working\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "\n",
    "\n",
    "predict('./balanced_independent_dataset.csv', \n",
    "        './balanced_independent_dataset_post_test.csv', \n",
    "        tokenizer_path, \n",
    "        '../model/trained_model/classifier-model',\n",
    "        1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confident and correct: 412\n",
      "confident: 503\n",
      "confident, viral and correct 177\n",
      "confident, human and correct 235\n",
      "total viral 613\n",
      "total human 613\n",
      "confident, viral 262\n",
      "confident, human 241\n",
      "not confident, viral 436\n",
      "not confident, human 378\n",
      "total rows: 1226\n",
      "Percentage of confident calls which were correct: 81.91%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Calculate the number of matches between 'classification' and 'predicted_class'\n",
    "a = pd.read_csv('./balanced_independent_dataset_post_test.csv')\n",
    "condition_1 = ((a['label'] == 1) & (a['viral_prob'] > 0.95)) | ((a['label'] == 0) & (a['viral_prob'] < 0.05))\n",
    "condition_2 = ((a['viral_prob'] > 0.95) | (a['viral_prob'] < 0.05))\n",
    "print('confident and correct:', condition_1.sum())\n",
    "print('confident:', condition_2.sum())\n",
    "\n",
    "\n",
    "condition_3 = ((a['label'] == 1) & (a['viral_prob'] > 0.95))\n",
    "condition_4 = (a['label'] == 0) & (a['viral_prob'] < 0.05)\n",
    "\n",
    "condition_5 = (a['label'] == 1)\n",
    "condition_6 = (a['label'] == 0)\n",
    "print('confident, viral and correct', condition_3.sum())\n",
    "print('confident, human and correct', condition_4.sum())\n",
    "print('total viral', condition_5.sum())\n",
    "print('total human',condition_6.sum())\n",
    "\n",
    "condition_7 =  (a['viral_prob'] > 0.95)\n",
    "condition_8 = (a['viral_prob'] < 0.05)\n",
    "print('confident, viral', condition_7.sum())\n",
    "print('confident, human', condition_8.sum())\n",
    "\n",
    "condition_9 = ((a['label'] == 1) & (a['viral_prob'] < 0.95))\n",
    "condition_10 = ((a['label'] == 0) & (a['viral_prob'] > 0.05))\n",
    "\n",
    "print('not confident, viral', condition_9.sum())\n",
    "print('not confident, human', condition_10.sum())\n",
    "matching_rows_count = condition_1.sum()\n",
    "confident_but_incorrect = condition_2.sum()\n",
    "total_rows_count = len(a)\n",
    "print('total rows:', total_rows_count)\n",
    "percentage = (matching_rows_count / confident_but_incorrect) * 100 if confident_but_incorrect > 0 else 0\n",
    "print(f\"Percentage of confident calls which were correct: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVQLVESGGGLVQPGGSLRLSCAASGFTFASYEMNWVRQAPGKGLEWISYISSRGSTTYYADSVKGRFTISRDNAKNSLYLQVNSLRAEDTAVYYCARDSPSSRGSPNLYYFDYWGQGTLVTVSS\n"
     ]
    }
   ],
   "source": [
    "from utils import get_full_aa_sub\n",
    "\n",
    "seq = \".....................................................................GCAGCCTCTGGATTCACCTTC............GCTAGTTATGAAATGAACTGGGTCCGCCAGGCTCCTGGGAAGGGGCTGGAGTGGATTTCATACATTAGTAGTCGT......GGTAGTACCACATACTACGCAGACTCTGTGAAG...GGCCGATTCACCATCTCCAGAGACAACGCCAAGAACTCACTGTATCTGCAAGTGAACAGCCTGAGAGCCGAGGACACGGCTGTTTATTACTGCGCGAGAGATAGCCCTTCCTCGCGGGGGAGCCCCAACCTGTACTACTTTGACTACTGGGGCCAGGGAACCCTGGTCACCGTCTCCTCA\"\n",
    "gadm = \"GAGGTGCAGCTGGTGGAGTCTGGGGGA...GGCTTGGTACAGCCTGGAGGGTCCCTGAGACTCTCCTGTGCAGCCTCTGGATTCACCTTC............AGTAGTTATGAAATGAACTGGGTCCGCCAGGCTCCAGGGAAGGGGCTGGAGTGGGTTTCATACATTAGTAGTAGT......GGTAGTACCATATACTACGCAGACTCTGTGAAG...GGCCGATTCACCATCTCCAGAGACAACGCCAAGAACTCACTGTATCTGCAAATGAACAGCCTGAGAGCCGAGGACACGGCTGTTTATTACTGTGCGAGAGANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNACTACTTTGACTACTGGGGCCAGGGAACCCTGGTCACCGTCTCCTCA\"\n",
    "print(get_full_aa_sub(seq, gadm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120800\n",
      "13923\n",
      "6149\n",
      "0.166158940397351\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "prepared_data = pd.read_csv('/home/ubuntu/data/FinalYearProject/machine_learning/data_preparation/SE_222_classified.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "viral_rows = prepared_data[prepared_data['human_viral_prediction'] == 'viral'].shape[0]\n",
    "human_rows = prepared_data[prepared_data['human_viral_prediction'] == 'human'].shape[0]\n",
    "\n",
    "total_rows = prepared_data.shape[0]\n",
    "print(total_rows)\n",
    "print(viral_rows)\n",
    "print(human_rows)\n",
    "print((viral_rows + human_rows)/total_rows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
